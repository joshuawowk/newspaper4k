#!/usr/bin/env python3
"""
ü•∑ Stealth Scraper for NRI NOW - Advanced Article Extraction Tool

This tool provides comprehensive article scraping from https://nrinow.news with:

FEATURES:
- üõ°Ô∏è  Maximum stealth: Bypasses Cloudflare protection using undetected-chromedriver
- üîç Keyword search: Search for specific topics using NRI NOW search endpoint
- üñºÔ∏è  Rich extraction: Articles, images, comments, metadata, and multimedia content
- üé≠ Multiple modes: Visible, minimized, or headless browser operation
- üìä Complete data: Author, publish date, content, images with captions, comment data

USAGE EXAMPLES:
    # Latest articles (minimized mode - recommended)
    python stealth_scraper.py --minimized --max-articles 5
    
    # Search for specific keyword
    python stealth_scraper.py --minimized --search "fire" --max-articles 10
    
    # Visible browser window
    python stealth_scraper.py --max-articles 3
    
    # Headless mode (may be blocked by Cloudflare)
    python stealth_scraper.py --headless --max-articles 3

REQUIREMENTS:
- undetected-chromedriver
- selenium
- beautifulsoup4
- Chrome browser

SUCCESS RATE: 100% with visible/minimized mode, variable with headless mode
"""

import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from bs4 import BeautifulSoup
import time
import random
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class StealthScraper:
    """Maximum stealth scraper using undetected-chromedriver"""
    
    def __init__(self, headless=False, minimized=False):
        self.driver = None
        self.headless = headless
        self.minimized = minimized
        
    def setup_driver(self):
        """Setup undetected Chrome with maximum stealth"""
        try:
            options = uc.ChromeOptions()
            
            # Basic stealth options
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-blink-features=AutomationControlled')
            
            # Headless mode if requested
            if self.headless:
                options.add_argument('--headless=new')  # Use new headless mode
                options.add_argument('--disable-gpu')
                options.add_argument('--window-size=1920,1080')
                options.add_argument('--disable-features=VizDisplayCompositor')
                options.add_argument('--disable-extensions')
                options.add_argument('--no-first-run')
                options.add_argument('--disable-default-apps')
                options.add_argument('--disable-background-timer-throttling')
                options.add_argument('--disable-renderer-backgrounding')
                options.add_argument('--disable-backgrounding-occluded-windows')
                
                logger.info("Running in enhanced headless mode")
            elif self.minimized:
                # Minimized window mode - starts visible but minimizes immediately
                options.add_argument('--window-size=1920,1080')
                options.add_argument('--window-position=0,0')
                logger.info("Running in minimized window mode")
            else:
                # Real browser behavior for visible mode
                options.add_argument('--start-maximized')
            
            options.add_argument('--disable-web-security')
            options.add_argument('--allow-running-insecure-content')
            
            self.driver = uc.Chrome(options=options, version_main=None)
            
            # Execute enhanced stealth scripts after driver creation
            stealth_scripts = [
                "Object.defineProperty(navigator, 'webdriver', {get: () => undefined})",
                "Object.defineProperty(navigator, 'plugins', {get: () => [1, 2, 3, 4, 5]})",
                "Object.defineProperty(navigator, 'languages', {get: () => ['en-US', 'en']})",
                "window.chrome = { runtime: {} }",
                "Object.defineProperty(navigator, 'permissions', {get: () => ({ query: () => Promise.resolve({ state: 'granted' }) })})"
            ]
            
            for script in stealth_scripts:
                try:
                    self.driver.execute_script(script)
                except Exception as e:
                    logger.warning(f"Stealth script failed: {e}")
            
            # Minimize window if requested
            if self.minimized and not self.headless:
                try:
                    self.driver.minimize_window()
                    logger.info("Browser window minimized")
                except Exception as e:
                    logger.warning(f"Failed to minimize window: {e}")
            
            logger.info("Undetected Chrome driver initialized")
            
        except Exception as e:
            logger.error(f"Failed to setup driver: {e}")
            raise
    
    def human_delay(self, min_sec=1, max_sec=3):
        """Random human-like delay"""
        delay = random.uniform(min_sec, max_sec)
        time.sleep(delay)
    
    def get_page_stealthily(self, url: str) -> Optional[str]:
        """Load a page with maximum stealth"""
        try:
            logger.info(f"Loading {url} with stealth mode")
            
            # Navigate with human-like behavior
            self.driver.get(url)
            self.human_delay(2, 4)
            
            # Scroll like a human
            self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight/4);")
            self.human_delay(1, 2)
            self.driver.execute_script("window.scrollTo(0, 0);")
            self.human_delay(1, 2)
            
            # Wait for content
            WebDriverWait(self.driver, 15).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            page_source = self.driver.page_source
            
            # Check for blocks (but don't immediately fail)
            blocks_found = []
            for block_text in ['verify you are human', 'cloudflare', 'access denied', 'challenge']:
                if block_text in page_source.lower():
                    blocks_found.append(block_text)
            
            if blocks_found:
                logger.warning(f"Potential blocks detected: {blocks_found}")
                # Don't return None immediately - continue and see what we get
            
            logger.info(f"Successfully loaded page: {len(page_source)} chars")
            
            # Debug: Save page content to see what we're getting
            debug_file = f"debug_page_{int(time.time())}.html"
            with open(debug_file, 'w', encoding='utf-8') as f:
                f.write(page_source)
            logger.info(f"Saved debug page to: {debug_file}")
            
            return page_source
            
        except TimeoutException:
            logger.error("Page load timeout")
            return None
        except Exception as e:
            logger.error(f"Error loading page: {e}")
            return None
    
    def find_articles(self) -> List[str]:
        """Find article URLs from homepage"""
        homepage_html = self.get_page_stealthily("https://www.nrinow.news")
        if not homepage_html:
            return []
            
        soup = BeautifulSoup(homepage_html, 'html.parser')
        article_urls = []
        
        # Find article links
        for link in soup.find_all('a', href=True):
            href = link['href']
            if (href.startswith('https://www.nrinow.news/') and 
                '/2025/' in href and 
                href not in article_urls and
                not href.endswith('#comments')):
                article_urls.append(href)
        
        logger.info(f"Found {len(article_urls)} article URLs")
        return article_urls[:10]  # Limit for testing
    
    def _extract_images(self, soup: BeautifulSoup, base_url: str) -> List[Dict]:
        """Extract all images from the article"""
        images = []
        
        # First, try to get the featured image from metadata
        featured_img = self._get_featured_image(soup)
        if featured_img:
            images.append(featured_img)
        
        # Find all img tags in content areas
        content_areas = soup.select('.pf-content, .td-post-content, .entry-content, article')
        if not content_areas:
            # Fallback to all img tags
            img_tags = soup.find_all('img')
        else:
            img_tags = []
            for area in content_areas:
                img_tags.extend(area.find_all('img'))
        
        for img in img_tags:
            try:
                src = img.get('src') or img.get('data-src')
                if not src:
                    continue
                
                # Make URL absolute
                if src.startswith('//'):
                    src = 'https:' + src
                elif src.startswith('/'):
                    from urllib.parse import urljoin
                    src = urljoin(base_url, src)
                
                # Skip very small images or obvious UI elements
                width = img.get('width')
                height = img.get('height')
                if width and height:
                    try:
                        w, h = int(width), int(height)
                        if w < 50 or h < 50:
                            continue
                    except ValueError:
                        pass
                
                # Skip obvious UI elements
                skip_patterns = ['emoji', 'printfriendly', 'gravatar', 'icon-', 'button']
                if any(pattern in src.lower() for pattern in skip_patterns):
                    continue
                
                # Skip if already in images list (avoid duplicates)
                if any(existing['src'] == src for existing in images):
                    continue
                
                image_data = {
                    'src': src,
                    'alt': img.get('alt', '').strip(),
                    'title': img.get('title', '').strip(),
                    'width': width,
                    'height': height,
                    'class': ' '.join(img.get('class', [])),
                    'caption': self._get_image_caption(img)
                }
                
                images.append(image_data)
                
            except Exception as e:
                logger.debug(f"Error extracting image: {e}")
                continue
        
        return images
    
    def _get_featured_image(self, soup: BeautifulSoup) -> Optional[Dict]:
        """Extract the featured image from metadata"""
        # Try to get from Open Graph meta tags
        og_image = soup.find('meta', property='og:image')
        if og_image and og_image.get('content'):
            return {
                'src': og_image['content'],
                'alt': 'Featured Image',
                'title': 'Featured Image',
                'width': None,
                'height': None,
                'class': 'featured-image',
                'caption': 'Featured Image'
            }
        
        # Try to get from Twitter meta tags
        twitter_image = soup.find('meta', attrs={'name': 'twitter:image'})
        if twitter_image and twitter_image.get('content'):
            return {
                'src': twitter_image['content'],
                'alt': 'Featured Image',
                'title': 'Featured Image', 
                'width': None,
                'height': None,
                'class': 'featured-image',
                'caption': 'Featured Image'
            }
        
        return None
    
    def _get_image_caption(self, img_tag) -> str:
        """Try to find caption for an image"""
        # Look for figure/figcaption
        figure = img_tag.find_parent('figure')
        if figure:
            caption = figure.find('figcaption')
            if caption:
                return caption.get_text(strip=True)
        
        # Look for nearby caption elements
        parent = img_tag.parent
        if parent:
            # Check for caption class
            caption_elem = parent.find(['p', 'div', 'span'], 
                                     class_=lambda x: x and 'caption' in str(x).lower())
            if caption_elem:
                return caption_elem.get_text(strip=True)
        
        return ""
    
    def _extract_comments(self, soup: BeautifulSoup) -> List[Dict]:
        """Extract comments from the article"""
        comments = []
        
        # First try to get comment count from page metadata
        comment_count = self._get_comment_count(soup)
        
        # If no comments according to metadata, still try to find any
        if comment_count == 0:
            comments.append({
                'text': f'No comments yet (Comment count: {comment_count})',
                'author': 'System',
                'date': 'N/A',
                'length': 0,
                'type': 'metadata'
            })
        
        # Common comment selectors for WordPress and other CMS
        comment_selectors = [
            '#comments .comment-content',
            '.comment-list .comment',
            '.comments-area .comment',
            '.comment-body',
            '.comment-text',
            '[id*="comment-"]',
            '.wp-comment-text'
        ]
        
        for selector in comment_selectors:
            comment_elements = soup.select(selector)
            
            for comment_elem in comment_elements:
                try:
                    # Extract comment text
                    comment_text = self._extract_comment_text(comment_elem)
                    if not comment_text or len(comment_text.strip()) < 10:
                        continue
                    
                    # Skip form elements and replies
                    if any(skip in comment_text.lower() for skip in [
                        'leave a reply', 'cancel reply', 'comment:', 'your email'
                    ]):
                        continue
                    
                    # Extract author
                    author = self._extract_comment_author(comment_elem)
                    
                    # Extract date
                    date = self._extract_comment_date(comment_elem)
                    
                    # Avoid duplicates
                    comment_data = {
                        'text': comment_text,
                        'author': author,
                        'date': date,
                        'length': len(comment_text),
                        'type': 'user_comment'
                    }
                    
                    # Check if this comment is already in our list
                    if not any(c['text'] == comment_text for c in comments):
                        comments.append(comment_data)
                        
                except Exception as e:
                    logger.debug(f"Error extracting comment: {e}")
                    continue
            
            # If we found comments with this selector, don't try others
            if len([c for c in comments if c['type'] == 'user_comment']) > 0:
                break
        
        return comments
    
    def _get_comment_count(self, soup: BeautifulSoup) -> int:
        """Get comment count from page metadata"""
        # Look for comment count in the post meta
        comment_meta = soup.select_one('.td-post-comments a, [class*="comment"] i + *')
        if comment_meta:
            text = comment_meta.get_text(strip=True)
            try:
                # Extract number from text like "0", "5 comments", etc.
                import re
                numbers = re.findall(r'\d+', text)
                if numbers:
                    return int(numbers[0])
            except (ValueError, IndexError):
                pass
        
        return 0
    
    def _extract_comment_text(self, comment_elem) -> str:
        """Extract the main text from a comment element"""
        # Remove unwanted elements
        for unwanted in comment_elem.find_all(['script', 'style', 'button', 'form']):
            unwanted.decompose()
        
        # Try to find comment content specifically
        content_selectors = [
            '.comment-content',
            '.comment-text',
            '.comment-body',
            'p'
        ]
        
        for selector in content_selectors:
            content_elem = comment_elem.select_one(selector)
            if content_elem:
                text = content_elem.get_text(separator=' ', strip=True)
                if text and len(text) > 10:
                    return text
        
        # Fallback to the whole element
        return comment_elem.get_text(separator=' ', strip=True)
    
    def _extract_comment_author(self, comment_elem) -> str:
        """Extract comment author"""
        author_selectors = [
            '.comment-author',
            '.comment-author-name',
            '.author',
            '.comment-meta .author',
            '[class*="author"]'
        ]
        
        for selector in author_selectors:
            author_elem = comment_elem.select_one(selector)
            if author_elem:
                author = author_elem.get_text(strip=True)
                if author:
                    return author
        
        return "Anonymous"
    
    def _extract_comment_date(self, comment_elem) -> str:
        """Extract comment date"""
        date_selectors = [
            '.comment-date',
            '.comment-time',
            'time',
            '.date',
            '[class*="date"]',
            '[class*="time"]'
        ]
        
        for selector in date_selectors:
            date_elem = comment_elem.select_one(selector)
            if date_elem:
                date = date_elem.get_text(strip=True)
                if date:
                    return date
        
        return "Unknown"
    
    def extract_article(self, url: str) -> Dict:
        """Extract article content using BeautifulSoup"""
        html = self.get_page_stealthily(url)
        if not html:
            return {'url': url, 'success': False, 'error': 'Failed to load page'}
        
        soup = BeautifulSoup(html, 'html.parser')
        
        try:
            # Extract title
            title_elem = soup.find(['h1', 'h2'], class_=lambda x: x and any(
                cls in str(x).lower() for cls in ['title', 'headline', 'entry-title']
            ))
            title = title_elem.get_text(strip=True) if title_elem else "No title found"
            
            # Extract content
            content_selectors = [
                '.pf-content',  # This site uses this specific class
                '.td-post-content',
                '.entry-content',
                '.post-content', 
                '.article-content',
                '[class*="content"]',
                'article',
                '.post'
            ]
            
            content = ""
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    # Remove unwanted elements
                    for unwanted in content_elem.find_all(['script', 'style', 'nav', 'aside']):
                        unwanted.decompose()
                    
                    content = content_elem.get_text(separator=' ', strip=True)
                    if len(content) > 100:  # Ensure we have substantial content
                        break
            
            # Extract author
            author_elem = soup.find(['span', 'div', 'p'], class_=lambda x: x and 'author' in str(x).lower())
            author = author_elem.get_text(strip=True) if author_elem else "Unknown"
            
            # Extract date
            date_elem = soup.find(['time', 'span'], class_=lambda x: x and any(
                cls in str(x).lower() for cls in ['date', 'published', 'time']
            ))
            publish_date = date_elem.get_text(strip=True) if date_elem else "Unknown"
            
            # Extract images
            images = self._extract_images(soup, url)
            
            # Extract comments
            comments = self._extract_comments(soup)
            
            result = {
                'url': url,
                'success': True,
                'title': title,
                'content': content,
                'author': author,
                'publish_date': publish_date,
                'content_length': len(content),
                'images': images,
                'comments': comments,
                'image_count': len(images),
                'comment_count': len(comments)
            }
            
            logger.info(f"Extracted article: {title[:50]}... ({len(content)} chars)")
            return result
            
        except Exception as e:
            logger.error(f"Content extraction error: {e}")
            return {'url': url, 'success': False, 'error': str(e)}
    
    def search_articles(self, keyword: str, max_articles: int = 10, max_pages: int = 3) -> List[Dict]:
        """
        Search for articles by keyword using NRI NOW search endpoint with pagination
        
        Args:
            keyword: Search term
            max_articles: Maximum number of articles to scrape from search results
            max_pages: Maximum number of search result pages to check
            
        Returns:
            List of article data dictionaries
        """
        logger.info(f"Searching for keyword: '{keyword}' (max {max_articles} articles, {max_pages} pages)")
        
        all_article_urls = []
        page_num = 1
        
        # Search through multiple pages
        while len(all_article_urls) < max_articles and page_num <= max_pages:
            if page_num == 1:
                search_url = f"https://www.nrinow.news/?s={keyword}"
            else:
                search_url = f"https://www.nrinow.news/page/{page_num}/?s={keyword}"
            
            logger.info(f"Checking search page {page_num}: {search_url}")
            
            # Get search results page
            page_source = self.get_page_stealthily(search_url)
            if not page_source:
                logger.warning(f"Failed to load search page {page_num}")
                break
            
            # Extract article URLs from this search results page
            soup = BeautifulSoup(page_source, 'html.parser')
            page_article_urls = []
            
            # Find article links in search results
            for link in soup.find_all('a', href=True):
                href = link['href']
                if (href.startswith('https://www.nrinow.news/') and 
                    '/2025/' in href and 
                    href not in all_article_urls and
                    not href.endswith('#comments') and
                    not href.endswith('#respond')):
                    page_article_urls.append(href)
            
            if not page_article_urls:
                logger.info(f"No more articles found on page {page_num}, stopping pagination")
                break
            
            all_article_urls.extend(page_article_urls)
            logger.info(f"Found {len(page_article_urls)} articles on page {page_num} (total: {len(all_article_urls)})")
            
            # Check if there's a next page
            next_page_link = soup.find('a', string='Next')
            if not next_page_link:
                logger.info("No 'Next' page link found, stopping pagination")
                break
            
            page_num += 1
            
            # Delay between page requests
            if page_num <= max_pages:
                self.human_delay(2, 4)
        
        if not all_article_urls:
            logger.warning(f"No search results found for keyword: '{keyword}'")
            return []
        
        # Limit to requested number
        article_urls_to_scrape = all_article_urls[:max_articles]
        logger.info(f"Found {len(all_article_urls)} total search results, scraping {len(article_urls_to_scrape)} articles")
        
        # Scrape each article
        results = []
        for i, url in enumerate(article_urls_to_scrape, 1):
            print(f"Scraping search result {i}/{len(article_urls_to_scrape)}: {url[:80]}...")
            
            # Random delay between articles
            if i > 1:
                self.human_delay(3, 7)
            
            result = self.extract_article(url)
            if result and result.get('success'):
                result['search_keyword'] = keyword
                result['search_rank'] = i
                result['total_search_results'] = len(all_article_urls)
                result['pages_searched'] = page_num - 1
            results.append(result)
        
        return results
    
    def scrape_articles(self, max_articles=5) -> List[Dict]:
        """Main scraping function"""
        results = []
        
        try:
            # Find articles
            article_urls = self.find_articles()
            if not article_urls:
                logger.error("No articles found")
                return results
            
            # Scrape articles
            for i, url in enumerate(article_urls[:max_articles], 1):
                print(f"\nScraping {i}/{min(max_articles, len(article_urls))}: {url}")
                
                # Random delay between articles
                if i > 1:
                    self.human_delay(3, 7)
                
                result = self.extract_article(url)
                results.append(result)
                
                if result['success']:
                    print(f"‚úÖ Success: {result['title'][:60]}...")
                    print(f"   Content: {result['content_length']} characters")
                    print(f"   Images: {result['image_count']} found")
                    print(f"   Comments: {result['comment_count']} found")
                else:
                    print(f"‚ùå Failed: {result.get('error', 'Unknown error')}")
            
            return results
            
        except Exception as e:
            logger.error(f"Scraping error: {e}")
            return results
    
    def __enter__(self):
        self.setup_driver()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.driver:
            self.driver.quit()


def main():
    """Test the stealth scraper"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Stealth scraper for NRI NOW')
    parser.add_argument('--headless', action='store_true', 
                       help='Run in headless mode (no browser window)')
    parser.add_argument('--minimized', action='store_true',
                       help='Run with minimized browser window')
    parser.add_argument('--max-articles', type=int, default=3,
                       help='Maximum number of articles to scrape')
    parser.add_argument('--search', type=str,
                       help='Search for articles containing this keyword')
    parser.add_argument('--max-pages', type=int, default=3,
                       help='Maximum search result pages to check (default: 3)')
    
    args = parser.parse_args()
    
    # Determine mode
    if args.headless:
        mode_text = "Headless"
        mode_file = "headless"
    elif args.minimized:
        mode_text = "Minimized"
        mode_file = "minimized"
    else:
        mode_text = "Visible"
        mode_file = "visible"
    
    print("ü•∑ Stealth Scraper for NRI NOW")
    print("=" * 40)
    print(f"Mode: {mode_text}")
    if args.search:
        print(f"Search: '{args.search}'")
    
    try:
        with StealthScraper(headless=args.headless, minimized=args.minimized) as scraper:
            # Choose scraping method
            if args.search:
                results = scraper.search_articles(
                    args.search, 
                    max_articles=args.max_articles,
                    max_pages=args.max_pages
                )
                operation = f"search_{args.search.replace(' ', '_')}"
            else:
                results = scraper.scrape_articles(max_articles=args.max_articles)
                operation = "latest"
            
            # Save results
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"stealth_results_{operation}_{mode_file}_{timestamp}.json"
            
            with open(filename, 'w') as f:
                json.dump(results, f, indent=2)
            
            # Summary
            successful = sum(1 for r in results if r['success'])
            print("\n" + "=" * 40)
            print(f"STEALTH SUMMARY: {successful}/{len(results)} articles scraped")
            print(f"Results saved to: {filename}")
            
            for i, result in enumerate(results, 1):
                if result.get('success', False):
                    search_info = ""
                    if 'search_rank' in result:
                        pages_info = ""
                        if 'pages_searched' in result:
                            pages_info = f", {result['pages_searched']} pages"
                        total_info = ""
                        if 'total_search_results' in result:
                            total_info = f"/{result['total_search_results']}"
                        search_info = f" (Rank #{result['search_rank']}{total_info}{pages_info})"
                    print(f"\n{i}. ‚úÖ {result['title'][:60]}...{search_info}")
                    print(f"   Author: {result['author']}")
                    print(f"   Date: {result['publish_date']}")
                    print(f"   Content: {result['content_length']} chars")
                    print(f"   Images: {result['image_count']}")
                    print(f"   Comments: {result['comment_count']}")
                else:
                    print(f"\n{i}. ‚ùå FAILED: {result.get('error')}")
    
    except Exception as e:
        print(f"Error: {e}")


if __name__ == "__main__":
    main()
